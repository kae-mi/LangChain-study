{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplate \n",
    "- 템플릿화된 프롬프트를 만들 수 있게 해주는 클래스\n",
    "- PromptTemplate() : 변수가 포함된 템플릿 객체를 만들 수 있게 해준다.\n",
    "    - template: 프롬프트 문자열. {country}는 나중에 값으로 대체될 변수.\n",
    "    - input_variables: 템플릿에서 사용할 변수명을 리스트로 지정해야 함.\n",
    "- invoke(): 템플릿에 실제 값을 넣어서, 사용할 문장을 만들어 줌.\n",
    "- 완성된 프롬프트를 LLM에 전달하여 응답을 받을 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-04T09:04:06.2541099Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3236246500, 'load_duration': 2695830200, 'prompt_eval_count': 32, 'prompt_eval_duration': 241230600, 'eval_count': 8, 'eval_duration': 298409100, 'model_name': 'llama3.2:1b'}, id='run--3dbba2d9-cc40-4892-add7-73b142650c6e-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is te capital of France?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-04T09:26:34.8439501Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1847780500, 'load_duration': 1400240100, 'prompt_eval_count': 32, 'prompt_eval_duration': 209836400, 'eval_count': 8, 'eval_duration': 236602600, 'model_name': 'llama3.2:1b'}, id='run--4e26fbd5-21b2-4ffd-bfde-0eb4f75aa4c1-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is te capital of {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain 의 메시지 클래스\n",
    "1.HumanMessage\n",
    "- BaseMessage를 상속받는다.\n",
    "- 사용자가 LLM에게 보내는 입력 메시지를 의미한다.\n",
    "- 대화형 시스템에서 사용자의 질문이나 지시를 표현할 때 사용된다.\n",
    "\n",
    "2.SystemMessage\n",
    "- BaseMessage를 상속받는다.\n",
    "- LLM의 동작 방식과 응답 스타일을 정의하는 지시사항을 담는다.\n",
    "- 대화 시작 시 LLM에게 역할, 제약조건, 목적 등을 알려줄 때 사용된다.\n",
    "\n",
    "3.AiMessage\n",
    "- BaseMessage를 상속받는다.\n",
    "- LLM이 생성한 응답을 나타낸다.\n",
    "- 대화 기록을 저장하거나 이전 AI 응답을 참조할 때 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 메시지들은 리스트로 감싸주어야 한다.\n",
    "\n",
    "아래와 같이 작성해서 실행하면 에러가 발생한다. \n",
    "- ValueError: Invalid input type <class 'langchain_core.messages.human.HumanMessage'>. Must be a PromptValue, str, or list of BaseMessages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'langchain_core.messages.human.HumanMessage'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rudah\\langchain-study\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:373\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    372\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m    374\u001b[39m             stop=stop,\n\u001b[32m    375\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    376\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    377\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    378\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    379\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    380\u001b[39m             **kwargs,\n\u001b[32m    381\u001b[39m         ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rudah\\langchain-study\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:358\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001b[32m    354\u001b[39m msg = (\n\u001b[32m    355\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    356\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    357\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'langchain_core.messages.human.HumanMessage'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke(HumanMessage(content=\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"What is the capital of France?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## message_list 를 만들고 그 안에 여러가지 메시지들을 만들어서 한번에 llm에 전달해보자.\n",
    "- 아래 예제를 보면 마치 LLM과 이전에 대화를 나눈것과 같은 느낌이 들게 한다.\n",
    "- 마치 대화를 이전에 나눴던 것처럼 LLM을 속이는 것이다.\n",
    "- 실제로 이런것을 fewshots(예제) 이라고 하는데 이렇게 에제를 넣어주는것이 LLM의 답변 생성 정확도에 긍정적인 영향을 끼친다는 논문이 있다.\n",
    "- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2005.14165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-04T09:37:47.1031037Z', 'done': True, 'done_reason': 'stop', 'total_duration': 930326300, 'load_duration': 17804100, 'prompt_eval_count': 62, 'prompt_eval_duration': 564410600, 'eval_count': 8, 'eval_duration': 346516300, 'model_name': 'llama3.2:1b'}, id='run--8506dbaf-9ccb-4f2b-be61-4f350348653b-0', usage_metadata={'input_tokens': 62, 'output_tokens': 8, 'total_tokens': 70})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplate\n",
    "ChatPromptTemplate은 LangChain에서 채팅 기반 언어 모델을 위한 프롬프트 템플릿이다.\n",
    "- 여러 메시지 유형(SystemMessage, HumanMessage, AIMessage)을 포함하는 템플릿을 생성할 수 있다.\n",
    "- 각 메시지에 변수 삽입을 할 수 있게 지원한다.\n",
    "- 채팅 모델이 기대하는 메시지 시퀀스 형식으로 구성한다.\n",
    "\n",
    "\n",
    "주의할 점\n",
    "- 메시지는 (역할, 내용) 형식의 튜플로 작성해야 한다.\n",
    "\n",
    "위 PromptTemplate 에서도 사용할 수 있는데 왜 이 방식을 사용하나?\n",
    "- LCEL에서 이 방식이 더 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-06-04T09:53:03.2824316Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1904374100, 'load_duration': 1372310400, 'prompt_eval_count': 38, 'prompt_eval_duration': 269322400, 'eval_count': 8, 'eval_duration': 261478900, 'model_name': 'llama3.2:1b'}, id='run--1acf7f82-4f51-43ee-a3ca-259893e4bae9-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"What is the capital of France?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"input\": \"France\"})\n",
    "\n",
    "llm.invoke(chat_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
